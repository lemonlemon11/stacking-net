{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = datasets.MNIST(root = \"./data/\",\n",
    "                            transform=transforms,\n",
    "                            train = True,\n",
    "                            download = True)\n",
    "\n",
    "data_test = datasets.MNIST(root=\"./data/\",\n",
    "                           transform = transforms,\n",
    "                           train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(file_name):\n",
    "    file_handle = open(file_name, \"rb\")  # 以二进制打开文档\n",
    "    file_content = file_handle.read()  # 读取到缓冲区中\n",
    "\n",
    "    head = struct.unpack_from('>II', file_content, 0)  # 取前2个整数，返回一个元组\n",
    "    offset = struct.calcsize('>II')\n",
    "\n",
    "    labelNum = head[1]  # label数\n",
    "    # print(labelNum)\n",
    "    bitsString = '>' + str(labelNum) + 'B'  # fmt格式：'>47040000B'\n",
    "    label = struct.unpack_from(bitsString, file_content, offset)  # 取data数据，返回一个元组\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_name):\n",
    "    #先用二进制方式把文件都读进来\n",
    "    file_handle=open(file_name,\"rb\")  #以二进制打开文档\n",
    "    file_content=file_handle.read()   #读取到缓冲区中\n",
    "\n",
    "    offset=0\n",
    "    head = struct.unpack_from('>IIII', file_content, offset)  # 取前4个整数，返回一个元组\n",
    "    offset += struct.calcsize('>IIII')\n",
    "    imgNum = head[1]  #图片数\n",
    "    rows = head[2]   #宽度\n",
    "    cols = head[3]  #高度\n",
    "    images=np.empty((imgNum , 784))#empty，是它所常见的数组内的所有元素均为空，没有实际意义，它是创建数组最快的方法\n",
    "    image_size=rows*cols#单个图片的大小\n",
    "    fmt='>' + str(image_size) + 'B'#单个图片的format\n",
    "\n",
    "    for i in range(imgNum):\n",
    "        images[i] = np.array(struct.unpack_from(fmt, file_content, offset))\n",
    "        # images[i] = np.array(struct.unpack_from(fmt, file_content, offset)).reshape((rows, cols))\n",
    "        offset += struct.calcsize(fmt)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import struct\n",
    "train_image = \"data/MNIST/raw/train-images-idx3-ubyte\"\n",
    "test_image = \"data/MNIST/raw/t10k-images-idx3-ubyte\"\n",
    "train_label = \"data/MNIST/raw/train-labels-idx1-ubyte\"\n",
    "test_label = \"data/MNIST/raw/t10k-labels-idx1-ubyte\"\n",
    "train_x = read_image(train_image)  # train_dataSet\n",
    "test_x = read_image(test_image)  # test_dataSet\n",
    "train_y = read_label(train_label)  # train_label\n",
    "test_y = read_label(test_label)  # test_label\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def knn2(train_idx,test_idx,k):\n",
    "    train_matrix=train_x[train_idx]\n",
    "    test_matrix=train_x[test_idx]\n",
    "    predict_label=[]\n",
    "    for test_data in test_matrix:\n",
    "        class_count=[0]*10\n",
    "        distance1=np.tile(test_data,(train_matrix.shape[0],1))-train_matrix\n",
    "        distance2=distance1**2\n",
    "        distance3=distance2.sum(axis=1)\n",
    "        distance4=distance3**0.5\n",
    "        sortedDistIndicies=distance4.argsort()\n",
    "        \n",
    "        for i in range(k):\n",
    "            label=train_y[sortedDistIndicies[i]]\n",
    "            class_count[label]+=1\n",
    "        max=0\n",
    "        id=0\n",
    "        for idx in range(len(class_count)):\n",
    "            if class_count[idx]>=max:\n",
    "                id=idx\n",
    "                max=class_count[idx]\n",
    "        predict_label.append(id)\n",
    "    \n",
    "    acc=accuracy_score(train_y[test_idx],predict_label)\n",
    "    \n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df=pd.read_csv('train.csv')\n",
    "test_df=pd.read_csv('test.csv')\n",
    "train_x=train_df.values[:,1:]\n",
    "train_y=train_df.values[:,0]\n",
    "test_x=test_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn(train_idx,test_idx,neighbor):\n",
    "    model = KNeighborsClassifier(n_neighbors=neighbor)\n",
    "    model.fit(train_x[train_idx], train_y[train_idx])\n",
    "    pre=model.predict(train_x[test_idx])\n",
    "#     print(accuracy_score(pre,train_y[test_idx]))\n",
    "    test_pre=model.predict(test_x)\n",
    "    return pre,test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def ramdom_forest(train_idx,test_idx,num_of_trees):\n",
    "    model = RandomForestClassifier(n_estimators=num_of_trees)\n",
    "    model.fit(train_x[train_idx], train_y[train_idx])\n",
    "\n",
    "    pre = model.predict(train_x[test_idx])\n",
    "#     acc_rf = accuracy_score(train_y[test_idx],y_pred_rf)\n",
    "    test_pre=model.predict(test_x)\n",
    "    return pre,test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "class fc_net(t.nn.Module):\n",
    "    '''\n",
    "    全连接网络\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(fc_net,self).__init__()\n",
    "        self.fc1 = t.nn.Sequential(t.nn.Linear(784,200),t.nn.ReLU())\n",
    "        self.fc2 = t.nn.Sequential(t.nn.Linear(200,100),t.nn.ReLU())\n",
    "        self.fc3 = t.nn.Sequential(t.nn.Linear(100,20),t.nn.ReLU())\n",
    "        self.fc4 = t.nn.Linear(20,10)\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "epochs=5\n",
    "BATCH_SIZE=10\n",
    "def conv_net(train_set,test_set):\n",
    "    net = fc_net()\n",
    "    criterion = t.nn.CrossEntropyLoss()\n",
    "    optim = t.optim.Adam(net.parameters(),lr=0.001,weight_decay=0.0)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        index=0\n",
    "        train_loss = 0   \n",
    "        train_acc = 0 \n",
    "        for i in tqdm(range(int(len(train_set)/BATCH_SIZE)),total=int(len(train_set)/BATCH_SIZE)):\n",
    "            x=train_x[train_set][index:index+BATCH_SIZE]\n",
    "            y=train_y[train_set][index:index+BATCH_SIZE]\n",
    "            \n",
    "            x=t.tensor(x,dtype=torch.float32)\n",
    "            y=t.tensor(y).long()\n",
    "            \n",
    "            out=net.forward(x)\n",
    "            loss=criterion(out,y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            index+=BATCH_SIZE\n",
    "            _,pred = out.max(1)\n",
    "            num_correct = (pred == y).sum()\n",
    "            acc = int(num_correct) / y.shape[0]\n",
    "            train_acc+=acc\n",
    "            \n",
    "#         print('acc', train_acc/len(train_set)*BATCH_SIZE)\n",
    "    \n",
    "    prediction=net(t.tensor(train_x[test_set],dtype=torch.float32))\n",
    "    _,pos=prediction.max(1)\n",
    "    num_correct = (t.tensor(train_y[test_set]).long() == pos).sum()\n",
    "    acc = int(num_correct) / len(test_set)\n",
    "    print(acc)\n",
    "    prediction_test_x=net(t.tensor(test_x,dtype=torch.float32))\n",
    "    _,pos_test_x=prediction_test_x.max(1)\n",
    "    return pos.tolist(),pos_test_x.tolist()\n",
    "        \n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3360/3360 [09:56<00:00,  5.63it/s]\n",
      "100%|██████████| 3360/3360 [09:57<00:00,  5.62it/s]\n",
      "100%|██████████| 3360/3360 [09:56<00:00,  5.63it/s]\n",
      "100%|██████████| 3360/3360 [09:56<00:00,  5.63it/s]\n",
      "100%|██████████| 3360/3360 [09:55<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9542857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3360/3360 [09:50<00:00,  5.69it/s]\n",
      "100%|██████████| 3360/3360 [09:53<00:00,  5.66it/s]\n",
      "100%|██████████| 3360/3360 [12:06<00:00,  4.63it/s]\n",
      "100%|██████████| 3360/3360 [12:01<00:00,  4.66it/s]\n",
      "100%|██████████| 3360/3360 [10:49<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9485714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3360/3360 [10:03<00:00,  5.57it/s]\n",
      "100%|██████████| 3360/3360 [09:57<00:00,  5.62it/s]\n",
      "100%|██████████| 3360/3360 [09:57<00:00,  5.63it/s]\n",
      "100%|██████████| 3360/3360 [09:58<00:00,  5.62it/s]\n",
      "100%|██████████| 3360/3360 [10:01<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3360/3360 [13:34<00:00,  4.12it/s]\n",
      "100%|██████████| 3360/3360 [15:55<00:00,  3.52it/s]\n",
      "100%|██████████| 3360/3360 [12:49<00:00,  4.37it/s]  \n",
      "100%|██████████| 3360/3360 [11:22<00:00,  4.92it/s]\n",
      "100%|██████████| 3360/3360 [10:35<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9583333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3360/3360 [10:52<00:00,  5.15it/s]\n",
      "100%|██████████| 3360/3360 [10:30<00:00,  5.33it/s]\n",
      "100%|██████████| 3360/3360 [10:35<00:00,  5.29it/s]\n",
      "100%|██████████| 3360/3360 [10:34<00:00,  5.30it/s]\n",
      "100%|██████████| 3360/3360 [10:36<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9573809523809523\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "k_fold=5\n",
    "index_matrix=np.array([i for i in range(train_x.shape[0])])\n",
    "data_length=index_matrix.shape[0]\n",
    "\n",
    "np.random.shuffle(index_matrix)\n",
    "\n",
    "model_1_train_data=[]\n",
    "model_1_test_data=[]\n",
    "\n",
    "model_2_train_data=[]\n",
    "model_2_test_data=[]\n",
    "\n",
    "model_3_train_data=[]\n",
    "model_3_test_data=[]\n",
    "num_of_each_fold=int(data_length/k_fold)\n",
    "for i in range(0,data_length,num_of_each_fold):\n",
    "    test_set=index_matrix[i:i+num_of_each_fold]\n",
    "    train_set=list(set(index_matrix)-set(test_set))\n",
    "    \n",
    "    train_pre_1,test_pre_1=knn(train_set,test_set,3)\n",
    "    model_1_train_data+=(list(train_pre_1))\n",
    "    model_1_test_data.append(list(test_pre_1))\n",
    "    \n",
    "    train_pre_2,test_pre_2=ramdom_forest(train_set,test_set,3)\n",
    "    model_2_train_data+=(list(train_pre_2))\n",
    "    model_2_test_data.append(list(test_pre_2))\n",
    "    \n",
    "    train_pre_3,test_pre_3=conv_net(train_set,test_set)\n",
    "    model_3_train_data+=(list(train_pre_3))\n",
    "    model_3_test_data.append(list(test_pre_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def majority_voting(data):\n",
    "    l=[]\n",
    "    data=np.array(data).T\n",
    "    for idx in range(data.shape[0]):\n",
    "        result=Counter(data[idx]).most_common(1)[0][0]\n",
    "        \n",
    "        l.append(result)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_train_x_output=np.c_[model_1_train_data,model_2_train_data,model_2_train_data]\n",
    "layer_1_train_y_output=train_y[index_matrix]\n",
    "layer_1_test_x_output=np.c_[majority_voting(model_1_test_data),majority_voting(model_2_test_data),majority_voting(model_3_test_data)]\n",
    "# layer_1_test_y_output=test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def logistice_regression(train_set,test_set):\n",
    "    lr = LogisticRegression(multi_class='multinomial',solver=\"newton-cg\")\n",
    "    lr.fit(layer_1_train_x_output[train_set], layer_1_train_y_output[train_set])\n",
    "    pred_train_x=lr.predict(layer_1_train_x_output[test_set])\n",
    "    pred_test_x=lr.predict(layer_1_test_x_output)\n",
    "    return pred_train_x,pred_test_x\n",
    "#     acc=accuracy_score(pred,layer_1_train_y_output[test_set])\n",
    "#     print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def svm_model(train_set,test_set):\n",
    "    model = svm.SVC(gamma='scale', C=4.0, decision_function_shape='ovr', kernel='rbf')\n",
    "    model.fit(layer_1_train_x_output[train_set], layer_1_train_y_output[train_set])\n",
    "    pred_train_x = model.predict(layer_1_train_x_output[test_set])\n",
    "    pred_test_x=model.predict(layer_1_test_x_output)\n",
    "    return pred_train_x,pred_test_x\n",
    "#     acc=accuracy_score(pred,layer_1_train_y_output[test_set])\n",
    "#     print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_matrix_2=np.array([i for i in range(layer_1_train_x_output.shape[0])])\n",
    "num_of_each_fold_2=int(index_matrix_2.shape[0]/5)\n",
    "\n",
    "layer_2_model_1_train_data=[]\n",
    "layer_2_model_1_test_data=[]\n",
    "\n",
    "layer_2_model_2_train_data=[]\n",
    "layer_2_model_2_test_data=[]\n",
    "\n",
    "layer_2_model_3_train_data=[]\n",
    "layer_2_model_3_test_data=[]\n",
    "for i in range(0,index_matrix_2.shape[0],num_of_each_fold_2):\n",
    "    test_set_2=index_matrix_2[i:i+num_of_each_fold_2]\n",
    "    train_set_2=list(set(index_matrix_2)-set(test_set_2))\n",
    "    \n",
    "    x,y=logistice_regression(train_set_2,test_set_2)\n",
    "    layer_2_model_1_train_data+=list(x)\n",
    "    layer_2_model_1_test_data.append(list(y))\n",
    "    \n",
    "    x1,y1=svm_model(train_set_2,test_set_2)\n",
    "    layer_2_model_2_train_data+=list(x1)\n",
    "    layer_2_model_2_test_data.append(list(y1))\n",
    "    \n",
    "    x2,y2=knn(train_set_2,test_set_2,3)\n",
    "    layer_2_model_3_train_data+=list(x2)\n",
    "    layer_2_model_3_test_data.append(list(y2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_train_x_output=np.c_[layer_2_model_1_train_data,layer_2_model_2_train_data,layer_2_model_3_train_data]\n",
    "layer_2_train_y_output=train_y[index_matrix_2]\n",
    "layer_2_test_x_output=np.c_[majority_voting(layer_2_model_1_test_data),majority_voting(layer_2_model_2_test_data),majority_voting(layer_2_model_3_test_data)]\n",
    "# layer_2_test_y_output=test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741785714285714"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=[list(np.unique(data)) for data in layer_2_test_x_output if np.unique(data).shape[0]!=1]\n",
    "# [i.shape[0] for i in r ]  \n",
    "1-np.array(r).shape[0]/len(test_x)\n",
    "# np.array(r).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model():\n",
    "    model = KNeighborsClassifier(n_neighbors=2)\n",
    "    model.fit(layer_2_train_x_output, layer_2_train_y_output)\n",
    "    pre=model.predict(layer_2_test_x_output)\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "def linear_regression(train_x,train_y):\n",
    "    \n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(train_x,train_y)\n",
    "    pred=reg.predict(layer_2_test_x_output)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred=final_model()\n",
    "# result=np.around(final_pred)\n",
    "# final_pred=majority_voting(layer_2_test_x_output.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df=pd.read_csv('sample_submission.csv')\n",
    "newDf = pd.DataFrame(submit_df, columns=['ImageId'])\n",
    "newDf['Label']=final_pred\n",
    "newDf.to_csv('stacking_net.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      9\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('stacking_net.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
